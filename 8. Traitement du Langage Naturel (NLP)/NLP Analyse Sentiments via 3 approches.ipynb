{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='steelblack'>\n",
    "    <h1 align=center> Analyse de sentiments (NLP) via 3 approches :</h1>\n",
    "    <h2 align=center>Sklearn simple, Pytorch simple, Pytorch-LSTM TORCH </h2>\n",
    "          </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=center>Cours conférence de Sébastien Collet - Data Scientist chez Saagie : Orchestrateur pour la DataOps</h2>\n",
    "\n",
    "<h3 align=center>Jean Martial Tagro - Data Scientist</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit de l'analyse d'un dataset de sentiments composé de quelques millions d'avis clients Amazon (texte d'entrée) et d'étoiles (étiquettes de sortie).\n",
    "Ce dataset constitue de vraies données commerciales à une échelle raisonnable mais peut être appris en un temps relativement court sur un ordinateur portable modeste. Dans le dataset, label 1 : sentiment positif ; label 2 : sentiment négatif.<br>\n",
    "Source : voir <a href='https://www.kaggle.com/bittlingmayer/amazonreviews?select=test.ft.txt.bz2'>Kaggle</a>\n",
    "<br>Evidement, les 3 approches étudiées dans ce Notebook sont indépendantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.tensor as tensor\n",
    "import torch.cuda as cuda\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3600000, 2), (400000, 2))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importation données source\n",
    "# engine='python' to take account sep > 1 char\n",
    "data_train = pd.read_csv('sentiment-train.txt', sep='##label##', header=None, names=['label','text'], engine='python')\n",
    "data_test  = pd.read_csv('sentiment-test.txt', sep='##label##', header=None, names=['label', 'text'], engine='python')\n",
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkred'>\n",
    "    <h1 align=center>Partie 1 : Approche Sklearn simple</h1>\n",
    "          </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vu la taille des données textuelles (4 millions de commentaires au total), nous allons dans un 1er temps faire l'étude sur un échantillon.\n",
    "\n",
    "Regardons les proportions de label avant le sampling :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 2]), array([1800000, 1800000]))\n",
      "(array([1, 2]), array([200000, 200000]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(data_train['label'].values, return_counts=True))\n",
    "print(np.unique(data_test['label'].values, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les proportions de sentiments positifs (2) et négatifs (1) sont égales. Obtenons de même un échantillons du dataset à proportions équivalentes de sentiments :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling (avec 1 entrée sur 100)\n",
    "\n",
    "data_train_sample = data_train.sample(frac=1/100)\n",
    "data_test_sample = data_test.sample(frac=1/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful - from Stack Overflow\n",
    "df.sample(frac=1)\n",
    "The frac keyword argument specifies the fraction of rows to return in the random sample, so frac=1 means return all rows (in random order).\n",
    "\n",
    "Note: If you wish to shuffle your dataframe in-place and reset the index, you could do e.g.\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "Here, specifying drop=True prevents .reset_index from creating a column containing the old index entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test Split\n",
    "X_train = data_train_sample['text'].values\n",
    "y_train = data_train_sample['label'].values\n",
    "\n",
    "X_test = data_test_sample['text'].values\n",
    "y_test = data_test_sample['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing des corpus de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<36000x68286 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1972308 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creation des vecteurs One Hot : Transform mots en features + Bag of Words (...)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_count = count_vect.fit_transform(X_train)\n",
    "X_train_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enorme matrice ! 36000 commentaires représentés par un vecteur de taille 68422 !\n",
    "#### Heureusement que l'objet Sparse matrix stocke en mémoire de façon plus intelligence des gros objets où il y a beaucoup de zéros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36000, 68286)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --> Pour une question de test rapide on va garder les milles features les plus fréquents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<36000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1437131 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creation des vecteurs One Hot : Transform mots en features + Bag of Words (...) -- avec une feature vector de taille 1000\n",
    "count_vect = CountVectorizer(max_features=1000)\n",
    "X_train_count = count_vect.fit_transform(X_train)\n",
    "X_train_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implémentation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jean-martial/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "\n",
    "classifier.fit(X_train_count, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prédiction sur le jeu de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations prealables\n",
    "X_test_count = count_vect.transform(X_test)  # Ne surtout pas (re)faire fit (...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score : 86.05 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score : {:.2f} %'.format(100*classifier.score(X_test_count, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    <h1 align=center>Partie 2 : Approche PyTorch simple</h1>\n",
    "          </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling (avec 1 entrée sur 100)\n",
    "\n",
    "data_train_sample = data_train.sample(frac=1/100)\n",
    "data_test_sample = data_test.sample(frac=1/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test Split\n",
    "X_train = data_train_sample['text'].values\n",
    "y_train = data_train_sample['label'].values\n",
    "\n",
    "X_test = data_test_sample['text'].values\n",
    "y_test = data_test_sample['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacement des labels 1-2 par 0-1\n",
    "y_train = np.where(y_train == 1, 0, 1)\n",
    "y_test = np.where(y_test == 1, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing des corpus de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<36000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1440155 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creation des vecteurs One Hot : Transform mots en features + Bag of Words (...) -- avec une feature vector de taille 1000\n",
    "count_vect = CountVectorizer(max_features=1000)\n",
    "X_train_count = count_vect.fit_transform(X_train)\n",
    "X_test_count = count_vect.transform(X_test)\n",
    "\n",
    "X_train_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformation des arrays numpy en tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_X_train = tensor(X_train_count.toarray(), dtype=torch.float)\n",
    "tensor_y_train = tensor(y_train, dtype=torch.long)\n",
    "\n",
    "tensor_X_test = tensor(X_test_count.toarray(), dtype=torch.float)\n",
    "tensor_y_test = tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implémentation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module): # nn.Module to let pyTorch know our class is a neural network\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(tensor_X_train.shape[1], 2)\n",
    "        #self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Perceptron().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fc1.weight', Parameter containing:\n",
      "tensor([[ 0.0112,  0.0252, -0.0315,  ..., -0.0284,  0.0121,  0.0236],\n",
      "        [-0.0066,  0.0059, -0.0177,  ..., -0.0208, -0.0309, -0.0010]],\n",
      "       requires_grad=True))\n",
      "('fc1.bias', Parameter containing:\n",
      "tensor([-0.0043, -0.0295], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# Check out initial parameters\n",
    "for param in classifier.named_parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Définitions : dataloader (batch), loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_trainset = TensorDataset(tensor_X_train, tensor_y_train) # fusion du jeu de train en 1 tenseur (text+label)\n",
    "dataloader_train = DataLoader(tensor_trainset, batch_size = 32, shuffle = True)\n",
    "\n",
    "tensor_testset = TensorDataset(tensor_X_test, tensor_y_test) # fusion du jeu de test en 1 tenseur (text+label)\n",
    "dataloader_test = DataLoader(tensor_testset, batch_size = 32, shuffle = True)\n",
    "\n",
    "# loss and optimizer definition\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = 0.01, weight_decay = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Boucle d'apprentissage du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 : loss = 0.5691317319869995\n",
      "epoch 2 : loss = 0.7858763337135315\n",
      "epoch 3 : loss = 0.6035353541374207\n",
      "epoch 4 : loss = 0.5927248001098633\n",
      "epoch 5 : loss = 0.6165134906768799\n",
      "epoch 6 : loss = 0.6241710186004639\n",
      "epoch 7 : loss = 0.7456296682357788\n",
      "epoch 8 : loss = 0.7697122097015381\n",
      "epoch 9 : loss = 0.6473281383514404\n",
      "epoch 10 : loss = 0.6206763982772827\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,11):\n",
    "    for batch in dataloader_train:\n",
    "        texts, labels = batch\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "    \n",
    "        outputs = classifier(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad() # to re-initialize gradient (not sum) after each batch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                         \n",
    "    print('epoch {} : loss = {}'.format(epoch, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prédictions sur le jeu de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score = 0.68075\n"
     ]
    }
   ],
   "source": [
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad(): # by default, pyTorch create graph to calculate gradients. -> No need here\n",
    "    for batch in dataloader_test:\n",
    "        texts, labels = batch\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "        outputs = classifier(texts)\n",
    "        _, predicted = torch.max(outputs, 1) # get the max values of both two output neurons - y axis\n",
    "        all_preds.append(predicted)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "print('accuracy score = {}'.format(accuracy_score(all_preds, all_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce simple perceptron est moins performant qu'une regression logistique.<br>\n",
    "Cette faible performance aussi est dûe qu'on a pas de Word Embedding (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkblue'>\n",
    "    <h1 align=center>Partie 3 : Approche PyTorch-LSTM-TORCHTEXT</h1>\n",
    "          </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtext contient plein de fonctions pour gestion de texte et sequences\n",
    "# pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports specifiques\n",
    "import torchtext\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Définition des pré-traitement sur le texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
